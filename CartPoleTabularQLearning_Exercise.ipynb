{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPoleTabularQLearning-Exercise.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Herding/Echo/blob/master/CartPoleTabularQLearning_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b84iUTHeAc0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#           _____                _____                    _____                    _____                    _____                    _____          \n",
        "#         /\\    \\              /\\    \\                  /\\    \\                  /\\    \\                  /\\    \\                  /\\    \\         \n",
        "#        /::\\    \\            /::\\    \\                /::\\    \\                /::\\    \\                /::\\    \\                /::\\    \\        \n",
        "#       /::::\\    \\           \\:::\\    \\              /::::\\    \\              /::::\\    \\              /::::\\    \\               \\:::\\    \\       \n",
        "#      /::::::\\    \\           \\:::\\    \\            /::::::\\    \\            /::::::\\    \\            /::::::\\    \\               \\:::\\    \\      \n",
        "#     /:::/\\:::\\    \\           \\:::\\    \\          /:::/\\:::\\    \\          /:::/\\:::\\    \\          /:::/\\:::\\    \\               \\:::\\    \\     \n",
        "#    /:::/__\\:::\\    \\           \\:::\\    \\        /:::/__\\:::\\    \\        /:::/__\\:::\\    \\        /:::/__\\:::\\    \\               \\:::\\    \\    \n",
        "#    \\:::\\   \\:::\\    \\          /::::\\    \\      /::::\\   \\:::\\    \\      /::::\\   \\:::\\    \\      /::::\\   \\:::\\    \\              /::::\\    \\   \n",
        "#  ___\\:::\\   \\:::\\    \\        /::::::\\    \\    /::::::\\   \\:::\\    \\    /::::::\\   \\:::\\    \\    /::::::\\   \\:::\\    \\    ____    /::::::\\    \\  \n",
        "# /\\   \\:::\\   \\:::\\    \\      /:::/\\:::\\    \\  /:::/\\:::\\   \\:::\\    \\  /:::/\\:::\\   \\:::\\____\\  /:::/\\:::\\   \\:::\\    \\  /\\   \\  /:::/\\:::\\    \\ \n",
        "#/::\\   \\:::\\   \\:::\\____\\    /:::/  \\:::\\____\\/:::/  \\:::\\   \\:::\\____\\/:::/  \\:::\\   \\:::|    |/:::/  \\:::\\   \\:::\\____\\/::\\   \\/:::/  \\:::\\____\\\n",
        "#\\:::\\   \\:::\\   \\::/    /   /:::/    \\::/    /\\::/    \\:::\\  /:::/    /\\::/   |::::\\  /:::|____|\\::/    \\:::\\  /:::/    /\\:::\\  /:::/    \\::/    /\n",
        "# \\:::\\   \\:::\\   \\/____/   /:::/    / \\/____/  \\/____/ \\:::\\/:::/    /  \\/____|:::::\\/:::/    /  \\/____/ \\:::\\/:::/    /  \\:::\\/:::/    / \\/____/ \n",
        "#  \\:::\\   \\:::\\    \\      /:::/    /                    \\::::::/    /         |:::::::::/    /            \\::::::/    /    \\::::::/    /          \n",
        "#   \\:::\\   \\:::\\____\\    /:::/    /                      \\::::/    /          |::|\\::::/    /              \\::::/    /      \\::::/____/           \n",
        "#    \\:::\\  /:::/    /    \\::/    /                       /:::/    /           |::| \\::/____/               /:::/    /        \\:::\\    \\           \n",
        "#     \\:::\\/:::/    /      \\/____/                       /:::/    /            |::|  ~|                    /:::/    /          \\:::\\    \\          \n",
        "#      \\::::::/    /                                    /:::/    /             |::|   |                   /:::/    /            \\:::\\    \\         \n",
        "#       \\::::/    /                                    /:::/    /              \\::|   |                  /:::/    /              \\:::\\____\\        \n",
        "#        \\::/    /                                     \\::/    /                \\:|   |                  \\::/    /                \\::/    /        \n",
        "#         \\/____/                                       \\/____/                  \\|___|                   \\/____/                  \\/____/         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbZN1xJ_oxPU",
        "colab_type": "text"
      },
      "source": [
        "# Cartpole Tabular Q Learning\n",
        "\n",
        "Docs:  \n",
        "https://github.com/openai/gym/wiki/CartPole-v0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8ViY2gqo2Ju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X-_KQXGoxPW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gjHH1Xns8Ks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5fq3VdNvRRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzChlVQAoxPZ",
        "colab_type": "text"
      },
      "source": [
        "# Play around with the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVAn7_MT7Ige",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = wrap_env(gym.make('CartPole-v0'))\n",
        "\n",
        "observations = []\n",
        "for i_episode in range(1):\n",
        "    observation = env.reset()\n",
        "    observations.append(observation)\n",
        "    for t in range(100):\n",
        "        env.render()\n",
        "        action = env.action_space.sample()\n",
        "        observation, reward, done, info = env.step(action)\n",
        "        \n",
        "        # record state, reward, done for printing\n",
        "        observations.append((observation, reward, done))\n",
        "        if done:\n",
        "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
        "            break\n",
        "env.close()\n",
        "for ob in observations: print(ob)\n",
        "show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ln5EwCGvoxPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(env.action_space)\n",
        "print(env.observation_space)\n",
        "print(env.observation_space.high)\n",
        "print(env.observation_space.low)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMyGpuHAoxPg",
        "colab_type": "text"
      },
      "source": [
        "# Discretisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBXMo-cXoxPg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.array([-0.1, 0.1, 2.4, 3.0, 4.6, 5.5, 6])  # some numbers that we want to see which bin they belong to\n",
        "bins = np.array(np.linspace(0., 5., 1)) # create 3 bins between 0 and 5\n",
        "out = np.digitize(x, bins)   # convert our inputs to the our bin values\n",
        "print(\"bins:\", bins.tolist())\n",
        "print(\"x:\", x.tolist())\n",
        "print(\"out:\", out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFkYjdCDoxPj",
        "colab_type": "text"
      },
      "source": [
        "# The Tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwibEjF1oxPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Epsilon(object):\n",
        "    def __init__(self, start=1.0, end=0.01, update_decrement=0.01):\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "        self.update_decrement = update_decrement\n",
        "        self._value = self.start\n",
        "        self.isTraining = True\n",
        "    \n",
        "    def decrement(self, count=1):\n",
        "        self._value = max(self.end, self._value - self.update_decrement*count)\n",
        "        return self\n",
        "        \n",
        "    def value(self):\n",
        "        if not self.isTraining:\n",
        "            return 0.0\n",
        "        else:\n",
        "            return self._value\n",
        "\"\"\"\n",
        "Instantiate object with epsilon starting at 1.0 (100% exploration), final value 0.01 (1% exploration), \n",
        "each time we call decrement it'll go down by 0.01. \n",
        "If eps.isTraining is set to True then it'll return 0.0 (zero exploration)\n",
        "\"\"\"\n",
        "eps = Epsilon(start=1.0, end=0.01, update_decrement=0.01)\n",
        "print(eps.value())\n",
        "print(\"decrementing 3 times\")\n",
        "print(eps.decrement().value())\n",
        "print(eps.decrement().value())\n",
        "print(eps.decrement().value())\n",
        "print(\"decrement 99 times and the lowest it goes to is 0.01\")\n",
        "print(eps.decrement(99).value())\n",
        "print(\"Set training = False\")\n",
        "eps.isTraining = False\n",
        "print(eps.decrement().value())\n",
        "print(\"Set training = True\")\n",
        "eps.isTraining = True\n",
        "print(eps.decrement().value())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aw5aEpZQoxPn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QTable():\n",
        "    def __init__(self, num_actions=4):\n",
        "        self.num_actions = num_actions\n",
        "        self.Q = {}\n",
        "    \n",
        "    \"\"\"Q(s, a): get the Q value of (s, a) pair\"\"\"\n",
        "    def get_Q(self, s, a):\n",
        "        self._check(s)\n",
        "        return self.Q[s][a]\n",
        "\n",
        "    \"\"\"if Q(s) doesn't exist initialize to 0\"\"\"\n",
        "    def _check(self, s):\n",
        "        if not s in self.Q:\n",
        "            self.Q[s] = [0.]*self.num_actions\n",
        "    \n",
        "    \"\"\"max Q(s): get the max of all Q value of state s\"\"\"\n",
        "    def get_max_Q(self, s):\n",
        "        self._check(s)\n",
        "        return np.max(self.Q[s])\n",
        "    \n",
        "    \"\"\"Q(s, a) = q: update the q value of (s, a) pair\"\"\"\n",
        "    def set_Q(self, s, a, q):\n",
        "        self._check(s)\n",
        "        self.Q[s][a] = q\n",
        "    \n",
        "    \"\"\"\n",
        "    argmax_a Q(s, a): get the action which has the highest Q in state s\n",
        "    - beware of the scenario where all Q[s] are equal, np.argmax just returns the first one\n",
        "    \"\"\"\n",
        "    def get_max_a_for_Q(self, s):\n",
        "        self._check(s)\n",
        "        if np.min(self.Q[s]) == np.max(self.Q[s]):\n",
        "          return random.choice([i for i in range(self.num_actions)]) # if all values are equal then do a random choice\n",
        "        else:\n",
        "          return np.argmax(self.Q[s])\n",
        "    \n",
        "    def __str__(self):\n",
        "        output = []\n",
        "        for s in self.Q:\n",
        "            output.append(s.__str__() + \": \" + [\"{:07.4f}\".format(a) for a in self.Q[s]].__str__())\n",
        "        output.sort()\n",
        "        return \"QTable (number of actions = \" + str(self.num_actions) + \", states = \" + str(len(output)) + \"):\\n\" + \"\\n\".join(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgShSdE_oxPp",
        "colab_type": "text"
      },
      "source": [
        "# Exercise: Q Learner for Cartpole\n",
        "### Suggested progression\n",
        "- Get agent training loop working with random action\n",
        "- Convert env state space into discretised states\n",
        "- Get agent action selection to use epsilon-greedy\n",
        "- Get agent to store and update Q values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnJ6_qajoxPp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent():\n",
        "    def __init__(self):\n",
        "        self.env = gym.make('CartPole-v0')\n",
        "        self.Q = QTable(num_actions=2)\n",
        "        self.epsilon = Epsilon(start=1.0, end=0.05, update_decrement=0.002)\n",
        "        \n",
        "    def getAction(self, s):\n",
        "        # TODO: implement epsilon-greedy policy\n",
        "        \n",
        "        action = self.env.action_space.sample()\n",
        "        return action\n",
        "    \n",
        "    def train(self, episodes=100):\n",
        "        self.epsilon.isTraining = True\n",
        "        # TODO: \n",
        "        # - Convert env state space into discretised states\n",
        "        # - Store and update Q values\n",
        "        pass\n",
        "                \n",
        "    \n",
        "    def run(self):\n",
        "        self.env = wrap_env(gym.make('CartPole-v0'))\n",
        "        self.epsilon.isTraining = False\n",
        "        s = self.env.reset()\n",
        "        steps = 0\n",
        "        while True:\n",
        "            self.env.render()\n",
        "            action = self.getAction(s)\n",
        "            s_1, reward, done, info = self.env.step(action)\n",
        "            steps += 1\n",
        "            if done:\n",
        "                print(\"Episode finished after {} timesteps\".format(steps))\n",
        "                break\n",
        "        self.env.close()\n",
        "        \n",
        "\n",
        "agent = Agent()\n",
        "agent.train(episodes=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo1yubsSQGVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent.run()\n",
        "show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}