{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lesson 4: NQL_skeleton.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Herding/Echo/blob/master/Lesson_4_NQL_skeleton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3t4ykp5zcSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#           _____                _____                    _____                    _____                    _____                    _____          \n",
        "#         /\\    \\              /\\    \\                  /\\    \\                  /\\    \\                  /\\    \\                  /\\    \\         \n",
        "#        /::\\    \\            /::\\    \\                /::\\    \\                /::\\    \\                /::\\    \\                /::\\    \\        \n",
        "#       /::::\\    \\           \\:::\\    \\              /::::\\    \\              /::::\\    \\              /::::\\    \\               \\:::\\    \\       \n",
        "#      /::::::\\    \\           \\:::\\    \\            /::::::\\    \\            /::::::\\    \\            /::::::\\    \\               \\:::\\    \\      \n",
        "#     /:::/\\:::\\    \\           \\:::\\    \\          /:::/\\:::\\    \\          /:::/\\:::\\    \\          /:::/\\:::\\    \\               \\:::\\    \\     \n",
        "#    /:::/__\\:::\\    \\           \\:::\\    \\        /:::/__\\:::\\    \\        /:::/__\\:::\\    \\        /:::/__\\:::\\    \\               \\:::\\    \\    \n",
        "#    \\:::\\   \\:::\\    \\          /::::\\    \\      /::::\\   \\:::\\    \\      /::::\\   \\:::\\    \\      /::::\\   \\:::\\    \\              /::::\\    \\   \n",
        "#  ___\\:::\\   \\:::\\    \\        /::::::\\    \\    /::::::\\   \\:::\\    \\    /::::::\\   \\:::\\    \\    /::::::\\   \\:::\\    \\    ____    /::::::\\    \\  \n",
        "# /\\   \\:::\\   \\:::\\    \\      /:::/\\:::\\    \\  /:::/\\:::\\   \\:::\\    \\  /:::/\\:::\\   \\:::\\____\\  /:::/\\:::\\   \\:::\\    \\  /\\   \\  /:::/\\:::\\    \\ \n",
        "#/::\\   \\:::\\   \\:::\\____\\    /:::/  \\:::\\____\\/:::/  \\:::\\   \\:::\\____\\/:::/  \\:::\\   \\:::|    |/:::/  \\:::\\   \\:::\\____\\/::\\   \\/:::/  \\:::\\____\\\n",
        "#\\:::\\   \\:::\\   \\::/    /   /:::/    \\::/    /\\::/    \\:::\\  /:::/    /\\::/   |::::\\  /:::|____|\\::/    \\:::\\  /:::/    /\\:::\\  /:::/    \\::/    /\n",
        "# \\:::\\   \\:::\\   \\/____/   /:::/    / \\/____/  \\/____/ \\:::\\/:::/    /  \\/____|:::::\\/:::/    /  \\/____/ \\:::\\/:::/    /  \\:::\\/:::/    / \\/____/ \n",
        "#  \\:::\\   \\:::\\    \\      /:::/    /                    \\::::::/    /         |:::::::::/    /            \\::::::/    /    \\::::::/    /          \n",
        "#   \\:::\\   \\:::\\____\\    /:::/    /                      \\::::/    /          |::|\\::::/    /              \\::::/    /      \\::::/____/           \n",
        "#    \\:::\\  /:::/    /    \\::/    /                       /:::/    /           |::| \\::/____/               /:::/    /        \\:::\\    \\           \n",
        "#     \\:::\\/:::/    /      \\/____/                       /:::/    /            |::|  ~|                    /:::/    /          \\:::\\    \\          \n",
        "#      \\::::::/    /                                    /:::/    /             |::|   |                   /:::/    /            \\:::\\    \\         \n",
        "#       \\::::/    /                                    /:::/    /              \\::|   |                  /:::/    /              \\:::\\____\\        \n",
        "#        \\::/    /                                     \\::/    /                \\:|   |                  \\::/    /                \\::/    /        \n",
        "#         \\/____/                                       \\/____/                  \\|___|                   \\/____/                  \\/____/  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxtBw5y0zqE4",
        "colab_type": "text"
      },
      "source": [
        "Install depencencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bz6pGESeznaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6I84ojCzuzG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKakJ7iXz7hN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1mIW9W8z8MD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzFTH3-Ayqts",
        "colab_type": "text"
      },
      "source": [
        "# NQL Skeleton Code\n",
        "Imports, hyperparameters and environment setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL0ukLKKyqtx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "GAMMA = 0.9  # discount factor for target Q\n",
        "INITIAL_EPSILON = 0.6  # starting value of epsilon\n",
        "FINAL_EPSILON = 0.05  # final value of epsilon\n",
        "EPSILON_DECAY_STEPS = 1000\n",
        "TEST_FREQUENCY = 100  # Num episodes to run before visualizing test accuracy\n",
        "HIDDEN_NODES = 20\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "EPISODE = 200000  # Episode limitation\n",
        "STEP = 200  # Step limitation in an episode\n",
        "TEST = 10  # The number of tests to run every TEST_FREQUENCY episodes\n",
        "env = wrap_env(gym.make(ENV_NAME)) #wrapping the env to render as a video\n",
        "epsilon = INITIAL_EPSILON\n",
        "STATE_DIM = env.observation_space.shape[0]\n",
        "ACTION_DIM = env.action_space.n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUG2arLjyquH",
        "colab_type": "text"
      },
      "source": [
        "Define our inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir14IT4qyquL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Add shape argument to placeholder. Hint: for state_in it will be [None, STATE_DIM]\n",
        "state_in = tf.placeholder(\"float\", )\n",
        "action_in = tf.placeholder(\"float\", ) \n",
        "target_in = tf.placeholder(\"float\", )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7_AvYrvyquT",
        "colab_type": "text"
      },
      "source": [
        "Define our network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueg9wR9yyquX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Add the units args and activation functions for the h2 and q_values layers\n",
        "h1 = tf.layers.dense(state_in, HIDDEN_NODES, activation=tf.nn.relu)\n",
        "h2 = tf.layers.dense(h1, ...\n",
        "\n",
        "q_values = tf.layers.dense(h2, ...\n",
        "\n",
        "q_action = tf.reduce_sum(tf.multiply(q_values, action_in), reduction_indices=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDFsscRAyquf",
        "colab_type": "text"
      },
      "source": [
        "Define our loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkJbftzryquh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Define the loss in terms of target_in and q_action tensors\n",
        "loss = ...\n",
        "optimizer = tf.train.AdamOptimizer(0.0003).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYpdbw4Myqup",
        "colab_type": "text"
      },
      "source": [
        "Define our exploration policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3dy527Eyquu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NOTE: assumes all our previously defined vars are accessible\n",
        "# TODO: using the generate Q-values, implement an e-greedy exploration policy from earlier in the workshop.\n",
        "# the return value must be a 1-hot representation of the action\n",
        "def explore(state, epsilon):\n",
        "    Q_estimates = q_values.eval(feed_dict={\n",
        "        state_in: [state]\n",
        "    })\n",
        "    ...\n",
        "    return one_hot_action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnX8kMnJyqu6",
        "colab_type": "text"
      },
      "source": [
        "Initialize Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo3QxYglyqu9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start session - Tensorflow housekeeping\n",
        "session = tf.InteractiveSession()\n",
        "session.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGGYDmFayqvM",
        "colab_type": "text"
      },
      "source": [
        "Now our main loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPzmsNwsyqvQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for episode in range(EPISODE):\n",
        "    # initialize task\n",
        "    state = env.reset()\n",
        "\n",
        "    # Update epsilon once per episode - linear schedule\n",
        "    epsilon -= epsilon / EPSILON_DECAY_STEPS\n",
        "\n",
        "    # Move through env according to e-greedy policy\n",
        "    for step in range(STEP):\n",
        "        action = explore(state, epsilon)\n",
        "\n",
        "        #TODO: Take an env step. Hint: env.step() takes an integer representing an action\n",
        "        next_state, reward, done, _ = ...\n",
        "\n",
        "        #TODO: get the q_values of the next state. Hint: look how we get them in the explore funciton\n",
        "        nextstate_q_values = ...\n",
        "        \n",
        "        if done:\n",
        "            target = reward\n",
        "        else:\n",
        "            #TODO: Calculate the target value when episode is not done\n",
        "            target = ...\n",
        "\n",
        "        # Do our train step\n",
        "        session.run([optimizer], feed_dict={\n",
        "            target_in: [target],\n",
        "            action_in: [action],\n",
        "            state_in: [state]\n",
        "        })\n",
        "        \n",
        "        #TODO: Update the state and terminate episode if done\n",
        "        ...\n",
        "\n",
        "    # Test and view sample runs - can disable render to save time\n",
        "    if (episode % TEST_FREQUENCY == 0 and episode != 0):\n",
        "        total_reward = 0\n",
        "        for i in range(TEST):\n",
        "            state = env.reset()\n",
        "            for j in range(STEP):\n",
        "                env.render()\n",
        "                action = np.argmax(q_values.eval(feed_dict={\n",
        "                    state_in: [state]\n",
        "                    }))\n",
        "                state, reward, done, _ = env.step(action)\n",
        "                total_reward += reward\n",
        "                if done:\n",
        "                    break\n",
        "        ave_reward = total_reward / TEST\n",
        "        print('episode:', episode, 'epsilon:', epsilon, 'Average Reward:', ave_reward)\n",
        "env.close()\n",
        "\n",
        "show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}