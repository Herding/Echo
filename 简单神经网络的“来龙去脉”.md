# 简单神经网络的“来龙去脉”[^1]

> 众所周知，神经网络有两次传播：正向输出结果，反向修正权重。
> 我们可以摆脱数学的约束，来看看正向传播和反向传播中的某些关键。

## 正向传播
神经网络如何处理输入的原始数据并得到较为准确的结果？个人理解，通过神经网络节点之间的权重以及激活函数。
权重对数据做出了相关的“选择”，可以“分辩”传递的数据，“选择”更重要的数据。举个例子：

> 人看苹果时不太会说：苹果是甜的；人尝完苹果后不太会说：苹果是红的。
> 因为视觉会通过相关的神经细胞处理，味觉会通过相关的神经细胞处理

数据通过权重的选择后，输入某一节点并通过激活函数输入到下一层或输出层。

其中的激活函数，通过适当的调整之后，也可以“选择”数据对下一层的影响程度。举例子：

> 同样一个面试者，面对一个严苛的面试官，一个宽松的面试官。那么面试者进入复试的可能性也会不一样。

## 反向传播
此处的传播并非再是正向所传播的数据，而是误差值。因为初始化神经网络中权重的不准确，导致如上所述的数据选择错误。通过反向传播误差，用于对权重进行更正（也就是引入梯度下降算法的理由）。传播过程可以直观的理解为，误差值作为输入值，不断通过“正向传播”传入至下一层（不需要激活函数进行处理），直到求出所有节点的误差值。在该过程里中间层的误差，显然是难以表述的，因而难以定义其产生误差方式。一种解决思路是：某一误差值通过权重的大小将误差分配给不同节点；某一节点对不同的传入误差进行求和表示为该节点的误差。举例子：

> 两个玩家对哥布林分别发起一次攻击并使其受到伤害。玩家小明的单次攻击力大于玩家小亮，那么我们可以直观的认为哥布林的大部分伤害来自小明，而只有少部分来自小亮。
>
> 同场游戏中，小明和小亮又斩杀了哥布林的头领。那么，在游戏结束计算小明给所有哥布林的伤害时，就要分别计算对哥布林以及其头领的伤害并求和。

此时，我们考虑一下为什么出发？我们的目的是为了更正权重。

## 梯度下降算法为何混入其中

我们已经知道神经网络里存在误差，引起误差的是权重。这可以理解为：只要权重选择合适，那么误差就会最小。梯度下降算法正是用于求解最小值的。现在我们知道为什么引入梯度下降算法了，那如何利用它来更新权重？神经网络是一个复杂的函数表达式，我们可以从$y= kx$来理解。我们选择合适的$k$值便能找到合适的表达式进行分类或拟合。

$y = kx$，表示所需的函数

$ y' = k'x$，表示现有的函数

那么两者相减就是误差：$\Delta y = y - y' = (k-k')x = \Delta k x$

则$\Delta k = \frac{\Delta y}{x}$

最后，可以使用上面的表达式来接近所需的函数：$k_{new} = k' + \alpha \Delta k,\ y = k_{new}x$，其中的$\alpha$防止斜率变动过大。

类似上面将$k$作为自变量，由于复杂函数中没有$\Delta k = \frac{\Delta y}{x}$这样的明显的关系，因此，选择对误差函数中的某一权重求偏导数（类似$x = \frac{\Delta y}{\Delta k} = \frac{dy}{dk}$）用于更正权重。

因此，得到$W_{new} = W_{old} - \alpha\frac{\partial E}{\partial W}$，将得到的新权重用于正向传播得到结果，并再次使用反向传播更正权重，知道误差最小。

### 具体实现

输入数据不宜太小或太大，计算机会损失精度。（输入数据为零时，处理为$0.01$，输出不能得到$[0.0,\ 1.0]$则处理为$[0.01,\ 0.99]$）

关于随机初始权重，数学家的建议是：可以在一个节点传入链接数量平方根倒数的大致范围内随机采样。（每个节点具有n条传入链接，那么初始权重范围$[-\frac{1}{\sqrt n},\frac{1}{\sqrt n}]$）

代码实现流程[^2]：

1. 框架代码
2. 初始化网络（定义神经网络形状和尺寸）
3. 初始化权重
4. 训练网络（输出计算值并更新权重）
5. 准备测试数据并测试网络
6. 改进模型（调整学习率、多次运行、改变网络形状）

---

[^1]: 《Python神经网络编程》[英]塔里克.拉希德（Tariq Rashid）著, 林赐 译
[^2]: https://github.com/makeyourownneuralnetwork/makeyourownneuralnetwork/bolb/master/part2_neural_network_mnist_data.ipynb